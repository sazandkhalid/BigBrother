# ğŸ¥ Interview Emotion & Performance Analysis System

*A Computer Vision & Behavioral Analytics Project from Hoyalytics*

## ğŸ“Œ Overview

This project was developed as part of an **interview assessment for Hoyalytics**. The goal was to analyze recorded interviews and provide interviewees with **actionable feedback** on their performance using **computer vision and behavioral analytics**.

The system uses facial expression detection, body movement tracking, and audio cadence analysis to evaluate key emotional and engagement metrics throughout an interview.

---

## ğŸ§  Key Features

* ğŸ­ **Emotion Detection** via PyFeat facial landmark tracking
* ğŸ‘ï¸ **Eye Contact & Gaze Stability** using Haar Cascades
* âœ‹ **Body Movement & Gesture Analysis**
* â±ï¸ **Talking Speed & Silence Duration Metrics**
* ğŸ“Š **Engagement and Nervousness Scores**
* ğŸ§¾ **Curated Summary Reports** (e.g., % time spent smiling, nervous, disengaged)

---

## ğŸ”§ Tools & Technologies

| Category         | Stack                                |
| ---------------- | ------------------------------------ |
| Language         | Python 3.9                           |
| CV Libraries     | `OpenCV`, `PyFeat`, `dlib`           |
| Video Processing | `cv2`, `ffmpeg`, `moviepy`           |
| Data Handling    | `pandas`, `numpy`                    |
| Visualization    | `matplotlib`, `seaborn`              |
| Model Evaluation | Custom thresholds, empirical testing |

---

## ğŸ§ª How It Works

1. **Video Ingestion**

   * Accepts `.mp4` or `.avi` format
   * Frames are extracted at regular intervals

2. **Facial Feature Detection**

   * Uses PyFeat to detect AU (Action Units) and map them to emotions
   * Haar Cascades identify eyes, face, and hand movement patterns

3. **Behavioral Analysis**

   * Duration of speech vs silence
   * Detection of nervousness (e.g. lip biting, eye darting)
   * Engagement level computed from facial orientation and gestures

4. **Summary Generation**

   * Outputs a report with time percentages for detected emotions
   * Engagement and nervousness scoring system
   * Visuals showing facial activity over time

---

## ğŸ“ Folder Structure

```
interview-emotion-analysis/
â”‚
â”œâ”€â”€ data/                # Sample interview videos
â”œâ”€â”€ scripts/             # Feature extraction and analysis code
â”‚   â”œâ”€â”€ emotion_tracker.py
â”‚   â”œâ”€â”€ body_movement.py
â”‚   â””â”€â”€ engagement_score.py
â”œâ”€â”€ reports/             # Auto-generated feedback reports
â”œâ”€â”€ notebooks/           # Exploratory analysis and testing
â”œâ”€â”€ README.md            # Project overview
â””â”€â”€ requirements.txt     # Dependencies
```

---

## ğŸ“Š Sample Outputs

| Metric                   | Value      |
| ------------------------ | ---------- |
| Smiling (Duration %)     | 15.2%      |
| Nervous (Detected Peaks) | 8          |
| Talking Time             | 82%        |
| Eye Contact Score        | 91.4 / 100 |
| Engagement Level         | High       |

ğŸ“ˆ Emotion chart over time
ğŸ“ Gaze heatmap
ğŸ“ Text-based summary of tips and insights

---

## ğŸš€ Impact

âœ… Delivered a **functioning emotion analysis pipeline** under interview constraints
âœ… Showcased **CV and behavioral modeling** for real-world application
âœ… Demonstrated ability to build **insightful, measurable feedback systems**

---

## ğŸ‘¤ My Contributions

* Developed all core **feature extraction and processing scripts**
* Tuned emotion detection thresholds via empirical testing
* Built logic for **engagement & nervousness scoring**
* Conducted integration testing across modules

---

## ğŸ“ Notes

* All videos used were synthetic or consented samples
* This system is not intended for clinical or diagnostic use
* Requires \~30 FPS interview video for best performance

